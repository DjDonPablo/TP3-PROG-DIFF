{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Optimisation : Implémentation et Evaluation\n",
    "- Maël Reynaud\n",
    "- Alexandre Devaux-Rivière"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import de la classe Tensor depuis le dernier TP (TP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data * other.data, [self, other], '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "\n",
    "        out = Tensor(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def build_topo(self, visited=None, topo=None):\n",
    "        if self not in visited:\n",
    "            visited.add(self)\n",
    "            for child in self._prev:\n",
    "                child.build_topo(visited=visited, topo=topo)\n",
    "            topo.append(self)\n",
    "        return topo\n",
    "\n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        topo = self.build_topo(topo=topo, visited=visited)\n",
    "        print(topo)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail pour le TP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des jeux de données synthétiques pour l'évaluation\n",
    "\n",
    "### Données linaires\n",
    "\n",
    "Le jeu de données linéaire est généré à partir d’une relation linéaire entre les variables x et y avec l’ajout d’un bruit gaussien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données non linéaires\n",
    "\n",
    "Le jeu de données non linéaire est basé sur une relation quadratique entre x et y avec un bruit ajouté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0, 5, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation des algorithmes\n",
    "\n",
    "Préparation de la classe optimiseur pour l'implémentation de nos optimizer customisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "    def __init__(self, params, learning_rate):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params: list, learning_rate: float = 0.01):\n",
    "        super().__init__(params=params, learning_rate=learning_rate)\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Propagation (RMSProp)\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(Optimizer):\n",
    "    def __init__(self , params: list, learning_rate: float = 0.01):\n",
    "        super().__init__(params=params, learning_rate=learning_rate)\n",
    "        self.G = [np.zeros_like(param.data) for param in params]\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def step(self):\n",
    "        for index, param in enumerate(self.params):\n",
    "            self.G[index] += param.grad ** 2\n",
    "            coef = np.sqrt(self.G[index]) + self.epsilon\n",
    "            param -= (self.learning_rate / coef) * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params: list, learning_rate: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n",
    "        super().__init__(params=params, learning_rate=learning_rate)\n",
    "        self.epsilon = eps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = [np.zeros_like(param.data) for param in params]\n",
    "        self.v = [np.zeros_like(param.data) for param in params]\n",
    "\n",
    "    def step(self):\n",
    "        for index, param in enumerate(self.params):\n",
    "            self.m[index] = self.beta1 * self.m[index] + (1 - self.beta1) * param.grad\n",
    "            self.v[index] = self.beta2 * self.v[index] + (1 - self.beta2) * param.grad ** 2\n",
    "            coef = np.sqrt(self.v[index]) + self.epsilon\n",
    "            param -= self.learning_rate / coef * self.m[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params: list, learning_rate: float = 0.001 , beta1: float = 0.9, beta2: float = 0.999 , eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        super().__init__(params=params, learning_rate=learning_rate)\n",
    "        self.epsilon = eps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = [np.zeros_like(param.data) for param in params]\n",
    "        self.v = [np.zeros_like(param.data) for param in params]\n",
    "\n",
    "    def step(self):\n",
    "        for index, param in enumerate(self.params):\n",
    "            self.m[index] = self.beta1 * self.m[index] + (1 - self.beta1) * param.grad\n",
    "            self.v[index] = self.beta2 * self.v[index] + (1 - self.beta2) * param.grad ** 2\n",
    "            coef = np.sqrt(self.v[index]) + self.epsilon\n",
    "            param -= self.learning_rate / coef * self.m[index] - self.learning_rate * self.weight_decay * param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des Optimiseur\n",
    "\n",
    "### Fonctions de Perte\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 2)**2\n",
    "\n",
    "def f_nonconvexe(x):\n",
    "    return 3*x**2 - 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expérimentation\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_optim():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de réseau de Neurones\n",
    "\n",
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_nn(x, W1 , b1 , W2 , b2):\n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2\n",
    "    return y\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du réseau\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nn_optim ():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation des scheduler de taux d’apprentissage\n",
    "\n",
    "### LRScheduler\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self , optimizer , initial_lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRSchedulerOnPlateau\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedulerOnPlateau(LRScheduler):\n",
    "    def __init__(self , optimizer , initial_lr , patience =10, factor =0.1, min_lr =1e-6, mode=\"min\", threshold =1e-4):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
