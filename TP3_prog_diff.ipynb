{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Optimisation : Implémentation et Evaluation\n",
    "- Maël Reynaud\n",
    "- Alexandre Devaux-Rivière"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import de la classe Tensor depuis le dernier TP (TP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data * other.data, [self, other], '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "\n",
    "        out = Tensor(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def build_topo(self, visited=None, topo=None):\n",
    "        if self not in visited:\n",
    "            visited.add(self)\n",
    "            for child in self._prev:\n",
    "                child.build_topo(visited=visited, topo=topo)\n",
    "            topo.append(self)\n",
    "        return topo\n",
    "\n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        topo = self.build_topo(topo=topo, visited=visited)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail pour le TP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import List, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des jeux de données synthétiques pour l'évaluation\n",
    "\n",
    "### Données linaires\n",
    "\n",
    "Le jeu de données linéaire est généré à partir d’une relation linéaire entre les variables x et y avec l’ajout d’un bruit gaussien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données non linéaires\n",
    "\n",
    "Le jeu de données non linéaire est basé sur une relation quadratique entre x et y avec un bruit ajouté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0, 5, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation des algorithmes\n",
    "\n",
    "Préparation de la classe optimiseur pour l'implémentation de nos optimizer customisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "    def __init__(self, params: List[Tensor], lr: float):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.W = torch.nn.parameter.Parameter(torch.tensor(1.0))\n",
    "    self.b = torch.nn.parameter.Parameter(torch.tensor(0.0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.W * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimizer(optim: Optimizer, torch_optim: torch.optim.Optimizer, optim_parameters: Dict):\n",
    "  W = Tensor(1.0)\n",
    "  b = Tensor(0.0)\n",
    "  torch_model = LinearModel()\n",
    "\n",
    "  optimizer = optim([W, b], **optim_parameters)\n",
    "  torch_optimizer = torch_optim(torch_model.parameters(), **optim_parameters)\n",
    "\n",
    "  print(\"Linear dataset (3 * x + 5)\")\n",
    "  for _ in range(100):\n",
    "    for x, y in zip(x_linear, y_linear):\n",
    "      loss = (y - (W * x + b)) ** 2\n",
    "      torch_loss = (y - torch_model(x)) ** 2\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      torch_optimizer.zero_grad()\n",
    "\n",
    "      loss.backward()\n",
    "      torch_loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      torch_optimizer.step()\n",
    "\n",
    "  print(f\"W : {W.data}, b : {b.data}\")\n",
    "  print(f\"[PYTORCH] W : {torch_model.W.data}, b : {torch_model.b.data}\")\n",
    "\n",
    "\n",
    "  W = Tensor(1.0)\n",
    "  b = Tensor(0.0)\n",
    "  torch_model = LinearModel()\n",
    "\n",
    "  optimizer = optim([W, b], **optim_parameters)\n",
    "  torch_optimizer = torch_optim(torch_model.parameters(), **optim_parameters)\n",
    "\n",
    "  print(\"Non linear dataset (0.5 * x^2 - 4 * x)\")\n",
    "  for _ in range(100):\n",
    "    for x, y in zip(x_linear, y_nonlinear):\n",
    "      loss = (y - (W * x + b)) ** 2\n",
    "      torch_loss = (y - torch_model(x)) ** 2\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      torch_optimizer.zero_grad()\n",
    "\n",
    "      loss.backward()\n",
    "      torch_loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      torch_optimizer.step()\n",
    "\n",
    "  print(f\"W : {W.data}, b : {b.data}\")\n",
    "  print(f\"[PYTORCH] W : {torch_model.W.data}, b : {torch_model.b.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params: List[Tensor], lr: float = 0.01):\n",
    "        super().__init__(params=params, lr=lr)\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear dataset (3 * x + 5)\n",
      "W : 3.1124139870722645, b : 4.931713920861934\n",
      "[PYTORCH] W : 3.1124141216278076, b : 4.931710720062256\n",
      "Non linear dataset (0.5 * x^2 - 4 * x)\n",
      "W : -0.7479125849883421, b : 13.664536448952166\n",
      "[PYTORCH] W : -0.7479139566421509, b : 13.664549827575684\n"
     ]
    }
   ],
   "source": [
    "test_optimizer(SGD, torch.optim.SGD, {\"lr\": 0.001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Propagation (RMSProp)\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params: List[Tensor], lr: float = 0.001, weight_decay: float = 0.9, alpha: float = 0.99, eps: int = 1e-8):\n",
    "        super().__init__(params=params, lr=lr)\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [0] * len(params)\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "\n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.params):\n",
    "            gt = param.grad + self.weight_decay * param.data\n",
    "            self.v[i] = self.alpha * self.v[i] + (1 - self.alpha) * (param.grad ** 2) \n",
    "            param.data -= self.lr * gt / (np.sqrt(self.v[i]) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear dataset (3 * x + 5)\n",
      "W : 2.9699739947949975, b : 3.4696535126733967\n",
      "[PYTORCH] W : 2.970080852508545, b : 3.521855592727661\n",
      "Non linear dataset (0.5 * x^2 - 4 * x)\n",
      "W : -2.681837225663783, b : 4.731417453278267\n",
      "[PYTORCH] W : -2.684574842453003, b : 4.821386337280273\n"
     ]
    }
   ],
   "source": [
    "test_optimizer(RMSProp, torch.optim.RMSprop, {\"lr\": 0.001, \"weight_decay\": 0.9, \"alpha\": 0.99, \"eps\": 1e-8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(Optimizer):\n",
    "    def __init__(self , params: List[Tensor], lr: float = 0.01):\n",
    "        super().__init__(params=params, lr=lr)\n",
    "        self.G = [np.zeros_like(param.data) for param in params]\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def step(self):\n",
    "        for index, param in enumerate(self.params):\n",
    "            self.G[index] += param.grad ** 2\n",
    "            coef = np.sqrt(self.G[index]) + self.epsilon\n",
    "            param.data -= (self.lr / coef) * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear dataset (3 * x + 5)\n",
      "W : 1.1379994326592422, b : 0.07407535004631868\n",
      "[PYTORCH] W : 1.1380008459091187, b : 0.07407517731189728\n",
      "Non linear dataset (0.5 * x^2 - 4 * x)\n",
      "W : 0.8920318490186844, b : 0.08891015717408494\n",
      "[PYTORCH] W : 0.8920310735702515, b : 0.08890986442565918\n"
     ]
    }
   ],
   "source": [
    "test_optimizer(Adagrad, torch.optim.Adagrad, {\"lr\": 0.001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD IMPLEM\n",
    "\n",
    "# class Adam(Optimizer):\n",
    "#     def __init__(self, params: list, lr: float = 0.001, betas: float = (0.9, 0.999), eps: float = 1e-8):\n",
    "#         super().__init__(params=params, lr=lr)\n",
    "#         self.epsilon = eps\n",
    "#         self.beta1 = betas[0]\n",
    "#         self.beta2 = betas[1]\n",
    "#         self.m = [np.zeros_like(param.data) for param in params]\n",
    "#         self.v = [np.zeros_like(param.data) for param in params]\n",
    "# \n",
    "#     def step(self):\n",
    "#         for index, param in enumerate(self.params):\n",
    "#             self.m[index] = self.beta1 * self.m[index] + (1 - self.beta1) * param.grad\n",
    "#             self.v[index] = self.beta2 * self.v[index] + (1 - self.beta2) * (param.grad ** 2)\n",
    "#             coef = np.sqrt(self.v[index]) + self.epsilon\n",
    "#             param.data -= self.lr * self.m[index] / coef "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS PYTORCH IMPLEM\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params: list, lr: float = 0.001, betas: float = (0.9, 0.999), eps: float = 1e-8):\n",
    "        super().__init__(params=params, lr=lr)\n",
    "        self.timestep = 1\n",
    "        self.epsilon = eps\n",
    "        self.beta1 = betas[0]\n",
    "        self.beta2 = betas[1]\n",
    "        self.m = [np.zeros_like(param.data) for param in params]\n",
    "        self.v = [np.zeros_like(param.data) for param in params]\n",
    "\n",
    "    def step(self):\n",
    "        for index, param in enumerate(self.params):\n",
    "            self.m[index] = self.beta1 * self.m[index] + (1 - self.beta1) * param.grad\n",
    "            self.v[index] = self.beta2 * self.v[index] + (1 - self.beta2) * (param.grad ** 2)\n",
    "            mhat = self.m[index] / (1 - self.beta1 ** self.timestep)\n",
    "            vhat = self.v[index] / (1 - self.beta2 ** self.timestep)\n",
    "            coef = np.sqrt(vhat) + self.epsilon\n",
    "            param.data -= self.lr * mhat / coef \n",
    "        self.timestep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear dataset (3 * x + 5)\n",
      "W : 2.974818069941666, b : 4.70047297094164\n",
      "[PYTORCH] W : 2.974820613861084, b : 4.70046329498291\n",
      "Non linear dataset (0.5 * x^2 - 4 * x)\n",
      "W : -2.8413365377254745, b : 4.957100575280201\n",
      "[PYTORCH] W : -2.841322422027588, b : 4.957065105438232\n"
     ]
    }
   ],
   "source": [
    "test_optimizer(Adam, torch.optim.Adam, {\"lr\": 0.001, \"betas\": (0.9, 0.999), \"eps\": 1e-08})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params: list, lr: float = 0.001 , beta1: float = 0.9, beta2: float = 0.999 , eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        super().__init__(params=params, lr=lr)\n",
    "        self.epsilon = eps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = [np.zeros_like(param.data) for param in params]\n",
    "        self.v = [np.zeros_like(param.data) for param in params]\n",
    "\n",
    "    def step(self):\n",
    "        for index, param in enumerate(self.params):\n",
    "            self.m[index] = self.beta1 * self.m[index] + (1 - self.beta1) * param.grad\n",
    "            self.v[index] = self.beta2 * self.v[index] + (1 - self.beta2) * param.grad ** 2\n",
    "            coef = np.sqrt(self.v[index]) + self.epsilon\n",
    "            param -= self.lr / coef * self.m[index] - self.lr * self.weight_decay * param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des Optimiseur\n",
    "\n",
    "### Fonctions de Perte\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 2)**2\n",
    "\n",
    "def f_nonconvexe(x):\n",
    "    return 3*x**2 - 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expérimentation\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_optim():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de réseau de Neurones\n",
    "\n",
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_nn(x, W1 , b1 , W2 , b2):\n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2\n",
    "    return y\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du réseau\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nn_optim ():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation des scheduler de taux d’apprentissage\n",
    "\n",
    "### LRScheduler\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self , optimizer , initial_lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRSchedulerOnPlateau\n",
    "\n",
    "TODO EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedulerOnPlateau(LRScheduler):\n",
    "    def __init__(self , optimizer , initial_lr , patience =10, factor =0.1, min_lr =1e-6, mode=\"min\", threshold =1e-4):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO EXPLAIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
